<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>源码 on Journey-C</title>
    <link>https://journey-c.github.io/tags/%E6%BA%90%E7%A0%81/</link>
    <description>Recent content in 源码 on Journey-C</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 16 May 2021 23:10:11 +0800</lastBuildDate>
    
	<atom:link href="https://journey-c.github.io/tags/%E6%BA%90%E7%A0%81/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Redis的网络模型</title>
      <link>https://journey-c.github.io/redis-network-model/</link>
      <pubDate>Sun, 16 May 2021 23:10:11 +0800</pubDate>
      
      <guid>https://journey-c.github.io/redis-network-model/</guid>
      <description>服务端程序几乎都会有网络交互的功能，一个优秀网络模型可以合理配合使用计算机的各资源。
 Redis作为广为人知的内存数据库，从玩具级项目到工业级项目中都可以看到它的身影，而Redis在最初的几个版本中一直是单线程，却能扛住1 million requests per second的请求量（非单点）。其实现的单线程网络模型必然十分优秀。 设计原理 在分析网络模型之前先分析一下Redis中网络交互的场景。一般来说我们在使用Redis时，一般会和Redis-Server建若干个连接，然后并发的给Redis-Server发送指令并得到回复。而Redis-Server就需要同时维护若干个与Redis-Client的连接，并且随时处理每个连接发来的请求。
一种方式是起一个线程监听一个端口，当新连接到来时，创建一个新线程处理这个连接。这样做的缺点是，当连接过多时线程数也随之增多，线程栈大小一般8MB，大量的线程会占用大量内存和CPU资源。
另一种方式是起一个线程监听端口，新连接交给线程池来处理，这样做的优点是连接数不再会压垮计算机，而缺点就是服务器的处理能力受限与线程池的大小，并且空闲连接也会占用线程池的资源。
上边两种网络模型的问题就在于一个线程只处理一个连接，而操作系统提供的IO多路复用技术可以解决这一问题。一个线程监听多个连接，每个连接只有在活跃时才会使用CPU，从而达到节省资源的目录。
Redis采用Reactor模式实现的网络模型。主要由事件收集器、事件发送器、事件处理器组成。事件收集器主要收集所有事件，包括来自硬件软件的事件。事件发送器负责将事件发送到实现注册的事件处理器。而事件处理器则负责处理事件。其中事件收集器就是通过IO多路复用技术来实现的。
数据结构 结构体aeEventLoop封装了事件循环相关的变量，包括两种事件的链表(时间事件、文件事件)。然后文件事件（aeFileEvent）中封装了读写事件接口充当事件处理器，时间事件（aeTimeEvent）中也封装了相应接口作为事件处理器。
事件 默认有两种事件：文件事件, 时间事件。
 文件事件对应文件的I/O事件，例如socket可读可写事件。 时间事件对应定时任务，例如Redis的定时清理等。  首先来看一下文件事件的封装。
/* File event structure */ typedef struct aeFileEvent { int mask; /* one of AE_(READABLE|WRITABLE|BARRIER) */ aeFileProc *rfileProc; aeFileProc *wfileProc; void *clientData; } aeFileEvent; 包含了一个标志位mask和read事件、write事件的处理器。如果文件事件对应的是客户端的话clientData就储存了对应connection接口。
时间事件就比较复杂，redis没有采用Time FD来实现定时任务，采用事件循环的timeout来辅助实现的。
/* Time event structure */ typedef struct aeTimeEvent { long long id; /* time event identifier. */ monotime when; aeTimeProc *timeProc; aeEventFinalizerProc *finalizerProc; void *clientData; struct aeTimeEvent *prev; struct aeTimeEvent *next; int refcount; /* refcount to prevent timer events from being * freed in recursive time event calls.</description>
    </item>
    
    <item>
      <title>Linux内存管理</title>
      <link>https://journey-c.github.io/linux-memory-management/</link>
      <pubDate>Fri, 19 Feb 2021 23:25:19 +0800</pubDate>
      
      <guid>https://journey-c.github.io/linux-memory-management/</guid>
      <description>计算机的计算，一方面说的是进程、线程对于CPU的使用，另一方面是对于内存的管理。本文就是介绍Linux的内存管理。
 在Linux中用户态是没有权限直接操作物理内存的，与硬件相关的交互都是通过系统调用由内核来完成操作的。Linux抽象出虚拟内存，用户态操作的只是虚拟内存，真正操作的物理内存由内核内存管理模块管理。本文通篇都在探索三个问题：
 虚拟地址空间是如何管理的 物理地址空间是如何管理的 虚拟地址空间和物理地址空间是如何映射的  上述三个问题得到解决之后，我们就可通过一个虚拟地址空间找到对应的物理地址空间。我们首先来看一下Linux虚拟地址空间的管理。
1. 虚拟地址空间的管理 是不是用户态使用虚拟内存，内核态直接使用物理内存呢？
 不是的，内核态和用户态使用的都是虚拟内存。
 使用虚拟地址一个核心的问题，需要记录虚拟地址到物理地址的映射，最简单的方式是虚拟地址与物理地址一一对应，这样4G内存光是维护映射关系就需要4G（扯淡）。所以需要其他有效的内存管理方案。通常有两种：分段、分页。下面我们来一起分析一下这两种管理机制以及在Linux中是如何应用的。
分段 8086升级到80386之后，段寄存器CS、DS、SS、ES从直接存放地址变成高位存放段选择子，低位做段描述符缓存器。由原来的直接使用内存地址变为现在的通过分段机制来使用内存地址。
那我们先来看一下内存管理中分段机制的原理。
分段机制下虚拟地址由两部分组成，段选择子和段内偏移量。段选择子中的段号作为段表的索引，通过段号可以在段表找到对应段表项，每一项记录了一段空间：段基址、段的界限、特权级等。用段基址+段内偏移量就可以计算出对应的物理地址。
Linux中段表称为段描述符表，放在全局描述符表中，用GDT_ENTRY_INIT函数来初始化表项desc_struct。
下面是Linux中段选择子和段表的定义，看一下所有段表项初始化传入的参数中，段基址base都是0，这没有分段。事实上Linux中没有用到全部的分段功能，对于内存管理更倾向于分页机制。
#define GDT_ENTRY_KERNEL32_CS	1 #define GDT_ENTRY_KERNEL_CS	2 #define GDT_ENTRY_KERNEL_DS	3  #define GDT_ENTRY_DEFAULT_USER32_CS	4 #define GDT_ENTRY_DEFAULT_USER_DS	5 #define GDT_ENTRY_DEFAULT_USER_CS	6 DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = { #ifdef CONFIG_X86_64 	[GDT_ENTRY_KERNEL32_CS]	= GDT_ENTRY_INIT(0xc09b, 0, 0xfffff), [GDT_ENTRY_KERNEL_CS]	= GDT_ENTRY_INIT(0xa09b, 0, 0xfffff), [GDT_ENTRY_KERNEL_DS]	= GDT_ENTRY_INIT(0xc093, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER32_CS]	= GDT_ENTRY_INIT(0xc0fb, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_DS]	= GDT_ENTRY_INIT(0xc0f3, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_CS]	= GDT_ENTRY_INIT(0xa0fb, 0, 0xfffff), #else 	[GDT_ENTRY_KERNEL_CS]	= GDT_ENTRY_INIT(0xc09a, 0, 0xfffff), [GDT_ENTRY_KERNEL_DS]	= GDT_ENTRY_INIT(0xc092, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_CS]	= GDT_ENTRY_INIT(0xc0fa, 0, 0xfffff), [GDT_ENTRY_DEFAULT_USER_DS]	= GDT_ENTRY_INIT(0xc0f2, 0, 0xfffff), .</description>
    </item>
    
    <item>
      <title>channel 源码阅读</title>
      <link>https://journey-c.github.io/channel-read/</link>
      <pubDate>Thu, 29 Oct 2020 20:58:31 +0800</pubDate>
      
      <guid>https://journey-c.github.io/channel-read/</guid>
      <description>年初的时候go语言的学习提上了日程，前一篇sync.pool阅读之后，阅读代码进度本该更快些，奈何身体被掏空，所以这篇文章断断续续一个月终于攒起来了。
 1. 简介 channel是golang中用于goroutine之间通讯的数据结构，有以下特点：
 线程安全 创建channel时返回的是指针，不需要考虑拷贝的问题 顺序通讯，写入和读出的顺序一致  2. 数据部分 源码位置go/src/runtime/chan.go
2.1 hchan channel对应的数据结构
type hchan struct { qcount uint dataqsiz uint buf unsafe.Pointer elemsize uint16 closed uint32 elemtype *_type sendx uint recvx uint recvq waitq sendq waitq // lock protects all fields in hchan, as well as several 	// fields in sudogs blocked on this channel. 	// 	// Do not change another G&amp;#39;s status while holding this lock 	// (in particular, do not ready a G), as this can deadlock 	// with stack shrinking.</description>
    </item>
    
    <item>
      <title>goroutine 源码阅读</title>
      <link>https://journey-c.github.io/golang-schedule/</link>
      <pubDate>Thu, 29 Oct 2020 20:58:31 +0800</pubDate>
      
      <guid>https://journey-c.github.io/golang-schedule/</guid>
      <description>1.数据结构 调度相关的数据结构有三个，M(线程)，P(调度器)，G(goroutine) M表示线程，P作为调度器用来帮助每个线程管理自己的goroutine，G就是golang的协程。我们可以通过runtime.GOMAXPROCS(n int)函数设置P的个数，注意P的个数并不代表M的个数，例如程序启动时runtime代码会出实话procs个P，但开始的时候只会启动一个M，就是M0和一个栈为64K(其他goroutine默认初始栈大小2K)来执行runtime代码。
那其他线程是什么时候创建的呐? 当goroutine被唤醒时，要在M上运行(恢复goroutine的上下文)，P是帮助M管理goroutine的，恢复上下文的操作也由P来完成。如果被唤醒时发现还有空闲的P，并且没有其他M在窃取goroutine(M发现本地goroutine队列和全局goroutine队列都没有goroutine的时候，会去其他线程窃取goroutine)，说明其他M都在忙，就会创建一个M让这个空闲的P帮他来管理goroutine。 总之一句话，开始的时候创建一个M，当发现调度不过来且还有空闲P没有工作就在创建新的，直到创建procs个M(procs通过runtime.GOMAXPROCS设置)
1.1 G golang 用结构体g表示goroutine
 g  type g struct { stack stack // 当前栈的范围[stack.lo, stack.hi) 	stackguard0 uintptr // 用于抢占的，一般情况值为stack.lo + StackGuard 	stackguard1 uintptr // 用于C语言的抢占 	_panic *_panic // 最内侧的panic函数 	_defer *_defer // 最外侧的defer函数 	m *m // 当前goroutine属于哪个m 	sched gobuf // 调度相关信息 	... schedlink guintptr // sched是全局的goroutine链表，schedlink表示这个goroutine在链表中的下一个goroutine的指针 	... preempt bool // 抢占标志，如果需要抢占就将preempt设置为true 	... }  gobuf gobuf保存goroutine的调度信息，当一个goroutine被调度的时，本质上就是把这个goroutine放到cpu，恢复各个寄存器的值，然后运行  type gobuf struct { sp uintptr // 栈指针 	pc uintptr // 程序计数器 	g guintptr // 当前被哪个goroutine持有 	ctxt unsafe.</description>
    </item>
    
    <item>
      <title>sync.pool 源码阅读</title>
      <link>https://journey-c.github.io/sync-pool-read/</link>
      <pubDate>Tue, 27 Oct 2020 20:58:31 +0800</pubDate>
      
      <guid>https://journey-c.github.io/sync-pool-read/</guid>
      <description>阅读项目代码的时候发现很多地方用到了golang的sync.pool，所以好奇golang的sync.pool底层实现是什么样的，有哪些优化。 本文是基于go1.13.10做讲解。
 在golang开发中sync.pool是最常用的缓存池，当一个对象被频繁创建和释放时会用到，但一般不作为连接池使用因为sync.pool中的对象随时会被释放掉，对象生命周期一般为两个GC间隔，且释放时机用户无感知。
1. 设计原理 sync.pool的操纵都是线程安全的，每个P都有自己私有的存储空间和共享的存储空间。
 GET 获取对象时，一般先在当前P的私有空间获取，如果没有，再到当前P的共享空间获取，如果还没有就窃取其他P的共享空间，如果还没有就访问上次GC遗留的对象。上述操作完成后还没有获取到，则调用New函数创建对象。 PUT 对象放回池子时，先判断当前P的私有空间是否为空，为空就放入，不为空就放入共享空间。  当GET/PUT非常频繁的时候，一般都只访问当前P的空间就可以完成操作。 GET/PUT不频繁时，即使访问到其他P的空间(有锁)，由于操作不频繁所以锁是可以接受的。
2. 数据结构 Pool是sync.Pool的核心数据结构。先了解一下该结构体的内部字段。
type Pool struct { noCopy noCopy local unsafe.Pointer // local fixed-size per-P pool, actual type is [P]poolLocal 	localSize uintptr // size of the local array  victim unsafe.Pointer // local from previous cycle 	victimSize uintptr // size of victims array  // New optionally specifies a function to generate 	// a value when Get would otherwise return nil.</description>
    </item>
    
  </channel>
</rss>