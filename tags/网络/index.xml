<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>网络 on Journey-C</title>
    <link>https://journey-c.github.io/tags/%E7%BD%91%E7%BB%9C/</link>
    <description>Recent content in 网络 on Journey-C</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 16 May 2021 23:10:11 +0800</lastBuildDate>
    
	<atom:link href="https://journey-c.github.io/tags/%E7%BD%91%E7%BB%9C/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Redis的网络模型</title>
      <link>https://journey-c.github.io/redis-network-model/</link>
      <pubDate>Sun, 16 May 2021 23:10:11 +0800</pubDate>
      
      <guid>https://journey-c.github.io/redis-network-model/</guid>
      <description>服务端程序几乎都会有网络交互的功能，一个优秀网络模型可以合理配合使用计算机的各资源。
 Redis作为广为人知的内存数据库，从玩具级项目到工业级项目中都可以看到它的身影，而Redis在最初的几个版本中一直是单线程，却能扛住1 million requests per second的请求量（非单点）。其实现的单线程网络模型必然十分优秀。 设计原理 在分析网络模型之前先分析一下Redis中网络交互的场景。一般来说我们在使用Redis时，一般会和Redis-Server建若干个连接，然后并发的给Redis-Server发送指令并得到回复。而Redis-Server就需要同时维护若干个与Redis-Client的连接，并且随时处理每个连接发来的请求。
一种方式是起一个线程监听一个端口，当新连接到来时，创建一个新线程处理这个连接。这样做的缺点是，当连接过多时线程数也随之增多，线程栈大小一般8MB，大量的线程会占用大量内存和CPU资源。
另一种方式是起一个线程监听端口，新连接交给线程池来处理，这样做的优点是连接数不再会压垮计算机，而缺点就是服务器的处理能力受限与线程池的大小，并且空闲连接也会占用线程池的资源。
上边两种网络模型的问题就在于一个线程只处理一个连接，而操作系统提供的IO多路复用技术可以解决这一问题。一个线程监听多个连接，每个连接只有在活跃时才会使用CPU，从而达到节省资源的目录。
Redis采用Reactor模式实现的网络模型。主要由事件收集器、事件发送器、事件处理器组成。事件收集器主要收集所有事件，包括来自硬件软件的事件。事件发送器负责将事件发送到实现注册的事件处理器。而事件处理器则负责处理事件。其中事件收集器就是通过IO多路复用技术来实现的。
数据结构 结构体aeEventLoop封装了事件循环相关的变量，包括两种事件的链表(时间事件、文件事件)。然后文件事件（aeFileEvent）中封装了读写事件接口充当事件处理器，时间事件（aeTimeEvent）中也封装了相应接口作为事件处理器。
事件 默认有两种事件：文件事件, 时间事件。
 文件事件对应文件的I/O事件，例如socket可读可写事件。 时间事件对应定时任务，例如Redis的定时清理等。  首先来看一下文件事件的封装。
/* File event structure */ typedef struct aeFileEvent { int mask; /* one of AE_(READABLE|WRITABLE|BARRIER) */ aeFileProc *rfileProc; aeFileProc *wfileProc; void *clientData; } aeFileEvent; 包含了一个标志位mask和read事件、write事件的处理器。如果文件事件对应的是客户端的话clientData就储存了对应connection接口。
时间事件就比较复杂，redis没有采用Time FD来实现定时任务，采用事件循环的timeout来辅助实现的。
/* Time event structure */ typedef struct aeTimeEvent { long long id; /* time event identifier. */ monotime when; aeTimeProc *timeProc; aeEventFinalizerProc *finalizerProc; void *clientData; struct aeTimeEvent *prev; struct aeTimeEvent *next; int refcount; /* refcount to prevent timer events from being * freed in recursive time event calls.</description>
    </item>
    
    <item>
      <title>backlog参数指的是什么？</title>
      <link>https://journey-c.github.io/what-is-the-backlog/</link>
      <pubDate>Sun, 31 Jan 2021 03:35:33 +0800</pubDate>
      
      <guid>https://journey-c.github.io/what-is-the-backlog/</guid>
      <description>背景介绍 服务端编程中涉及网络交互的服务几乎都会监听一个端口，然后等待客户端的请求，然后交互。在Linux中监听的系统调用是listen。glibc的接口如下：
int listen(int sockfd, int backlog); 其中参数sockfd为已经bind过端口和地址的fd，而backlog就是本文介绍的对象。
BSD手册中给它的定义是：
 &amp;ldquo;the maximum length the queue of pending connections may grow to.（由未处理连接构成的队列可能增长的最大长度）
 这句话并没有解释backlog到底是处于SYN_RCVD状态的连接数还是处于ESTABLISHED状态的连接数。或者是处于两者皆可。
Linux中的backlog是如何实现 下面我们从Linux实现来一步步揭开backlog的真面目。
首先listen涉及与网卡的交互，这种涉及与硬件交互的操作Linux都是通过系统调用来实现的，既然是系统调用那么目标就明确了，从listen的系统调用入口开始看。
listen函数的入口是SYSCALL_DEFINE2(listen, int, fd, int, backlog)参数正如glibc的listen接口，第一个参数是listen用的socket，第二个参数是backlog。这个函数没有做任何事情只是调用了__sys_listen，__sys_listen就是具体的listen实现了：
 首先根据传入的fd调用sockfd_lookup_light找到对应的socket对象。 将backlog和Linux配置中的somaxconn(/proc/sys/net/core/somaxconn，默认128)比较，如果比somaxconn大，就用somaxconn替换。 调用struct socket结构里面ops的listen函数，拿TCP来说，创建socket时type=SOCK_STREAM，protocol=IPPROTO_TCP的ops是inet_stream_ops，对应的listen函数是inet_listen。 inet_listen中判断一下socket状态还不是LISTEN的话，会调用inet_csk_listen_start进入监听状态。另外还会将backlog值赋给socket的sk_max_ack_backlog参数，后边虽然调用一直带着backlog参数，实际没用了，socket中已经有了。 inet_csk_listen_start中会创建一个新结构struct inet_connection_sock。这个结构体是维护连接状态的，里面包含了各种状态队列和超时以及拥塞控制的变量，其中我们关心的是icsk_accept_queue队列。内核会为每个socket维护两个队列，一个是三次握手完成处于ESTABLISHED状态的连接队列，另一个是三次握手进行中处于SYN_RCVD状态的连接队列，icsk_accept_queue就是前者。而用户调用accept实际是从icsk_accept_queue队列取出连接。 初始化完之后，将 TCP 的状态设置为 TCP_LISTEN，再次调用 get_port 判断端口是否冲突。listen的逻辑就结束了。  上面已经介绍完listen的整个逻辑了，与咱们讨论的backlog有关的就是icsk_accept_queue队列。
当内核收到网卡收到数据而触发的硬中断之后，并且数据传递到四层时：
 如果是ipv4的tcp包会调用tcp_v4_rcv，处理完tcp头以及其他一些信息之后就调用tcp_v4_do_rcv，这个函数中分两种情况：处于ESTABLISHED状态的socket和未处于ESTABLISHED状态的socket。 我们关心的是未处于ESTABLISHED状态的socket，会调用tcp_rcv_state_process，这个函数中，当socket状态是LISTEN时（因为客户端的连接包是发给listen fd的），会调用struct inet_connection_sock(listen系统调用时创建的)icsk_af_ops对象的conn_request接口，对应tcp_conn_request函数。 tcp_conn_request会调用inet_csk_reqsk_queue_is_full函数判断当前icsk_accept_queue长度是否超过sk_max_ack_backlog，如果超过就给客户端发一个RST包，客户端就当SYN包丢了，然后一直重试，第一次6s超时，然后24s，直到75s还没收到SYNACK就返回用户连接超时。  到目前为止得出结论，backlog是指用户未处理的连接数量，例如backlog为1，有三个客户端在同时连接，第一个连接可以正常三次握手，第二个连接SYN包到来时内核只会回一个RST包，客户端就当SYN包丢了不停重试，当用户调用accept获取了第一个连接之后，第二个内核才会给第二个连接回复SYNACK继续握手。
当然icsk_accept_queue最大长度不是绝对为backlog，而是backlog*模糊因子，下面是不同操作系统的backlog的设置。 图片转自《UNIX网络编程卷一》</description>
    </item>
    
    <item>
      <title>长连接平滑重启</title>
      <link>https://journey-c.github.io/long-connection-smooth-restart-realization/</link>
      <pubDate>Wed, 21 Oct 2020 22:48:31 +0800</pubDate>
      
      <guid>https://journey-c.github.io/long-connection-smooth-restart-realization/</guid>
      <description>最近小编一直在做长连接相关的事情，最大的感触就是发版太痛苦，一个个踢掉连接然后发版，导致发版时长过长，操作繁琐。所以在想能不能实现优雅重启, 发版时客户端无感知。
 1.难点   如何做到不中断接收连接
  如何做到已有连接不中断
  2.解决 2.1 如何做到不中断接受连接 以下是linux源码中bind的实现(linux-1.0)
// linux-1.0/net/socket.c 536 static int sock_bind(int fd, struct sockaddr *umyaddr, int addrlen) { struct socket *sock; int i; DPRINTF((net_debug, &amp;#34;NET: sock_bind: fd = %d\n&amp;#34;, fd)); if (fd &amp;lt; 0 || fd &amp;gt;= NR_OPEN || current-&amp;gt;filp[fd] == NULL) return(-EBADF); //获取fd对应的socket结构  if (!(sock = sockfd_lookup(fd, NULL))) return(-ENOTSOCK); // 转调用bind指向的函数，下层函数(inet_bind)  if ((i = sock-&amp;gt;ops-&amp;gt;bind(sock, umyaddr, addrlen)) &amp;lt; 0) { DPRINTF((net_debug, &amp;#34;NET: sock_bind: bind failed\n&amp;#34;)); return(i); } return(0); } // linux-1.</description>
    </item>
    
  </channel>
</rss>